{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7644a1f4-4af3-49fd-834a-f015361386b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9800 entries, 0 to 9799\n",
      "Data columns (total 18 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Row ID         9800 non-null   int64  \n",
      " 1   Order ID       9800 non-null   object \n",
      " 2   Order Date     9800 non-null   object \n",
      " 3   Ship Date      9800 non-null   object \n",
      " 4   Ship Mode      9800 non-null   object \n",
      " 5   Customer ID    9800 non-null   object \n",
      " 6   Customer Name  9800 non-null   object \n",
      " 7   Segment        9800 non-null   object \n",
      " 8   Country        9800 non-null   object \n",
      " 9   City           9800 non-null   object \n",
      " 10  State          9800 non-null   object \n",
      " 11  Postal Code    9789 non-null   float64\n",
      " 12  Region         9800 non-null   object \n",
      " 13  Product ID     9800 non-null   object \n",
      " 14  Category       9800 non-null   object \n",
      " 15  Sub-Category   9800 non-null   object \n",
      " 16  Product Name   9800 non-null   object \n",
      " 17  Sales          9800 non-null   float64\n",
      "dtypes: float64(2), int64(1), object(15)\n",
      "memory usage: 1.3+ MB\n",
      "None\n",
      "\n",
      "First 5 Rows:\n",
      "   Row ID        Order ID  Order Date   Ship Date       Ship Mode Customer ID  \\\n",
      "0       1  CA-2017-152156  08/11/2017  11/11/2017    Second Class    CG-12520   \n",
      "1       2  CA-2017-152156  08/11/2017  11/11/2017    Second Class    CG-12520   \n",
      "2       3  CA-2017-138688  12/06/2017  16/06/2017    Second Class    DV-13045   \n",
      "3       4  US-2016-108966  11/10/2016  18/10/2016  Standard Class    SO-20335   \n",
      "4       5  US-2016-108966  11/10/2016  18/10/2016  Standard Class    SO-20335   \n",
      "\n",
      "     Customer Name    Segment        Country             City       State  \\\n",
      "0      Claire Gute   Consumer  United States        Henderson    Kentucky   \n",
      "1      Claire Gute   Consumer  United States        Henderson    Kentucky   \n",
      "2  Darrin Van Huff  Corporate  United States      Los Angeles  California   \n",
      "3   Sean O'Donnell   Consumer  United States  Fort Lauderdale     Florida   \n",
      "4   Sean O'Donnell   Consumer  United States  Fort Lauderdale     Florida   \n",
      "\n",
      "   Postal Code Region       Product ID         Category Sub-Category  \\\n",
      "0      42420.0  South  FUR-BO-10001798        Furniture    Bookcases   \n",
      "1      42420.0  South  FUR-CH-10000454        Furniture       Chairs   \n",
      "2      90036.0   West  OFF-LA-10000240  Office Supplies       Labels   \n",
      "3      33311.0  South  FUR-TA-10000577        Furniture       Tables   \n",
      "4      33311.0  South  OFF-ST-10000760  Office Supplies      Storage   \n",
      "\n",
      "                                        Product Name     Sales  \n",
      "0                  Bush Somerset Collection Bookcase  261.9600  \n",
      "1  Hon Deluxe Fabric Upholstered Stacking Chairs,...  731.9400  \n",
      "2  Self-Adhesive Address Labels for Typewriters b...   14.6200  \n",
      "3      Bretford CR4500 Series Slim Rectangular Table  957.5775  \n",
      "4                     Eldon Fold 'N Roll Cart System   22.3680  \n",
      "\n",
      "Summary Statistics:\n",
      "            Row ID   Postal Code         Sales\n",
      "count  9800.000000   9789.000000   9800.000000\n",
      "mean   4900.500000  55273.322403    230.769059\n",
      "std    2829.160653  32041.223413    626.651875\n",
      "min       1.000000   1040.000000      0.444000\n",
      "25%    2450.750000  23223.000000     17.248000\n",
      "50%    4900.500000  58103.000000     54.490000\n",
      "75%    7350.250000  90008.000000    210.605000\n",
      "max    9800.000000  99301.000000  22638.480000\n",
      "\n",
      "Missing Values:\n",
      "Row ID            0\n",
      "Order ID          0\n",
      "Order Date        0\n",
      "Ship Date         0\n",
      "Ship Mode         0\n",
      "Customer ID       0\n",
      "Customer Name     0\n",
      "Segment           0\n",
      "Country           0\n",
      "City              0\n",
      "State             0\n",
      "Postal Code      11\n",
      "Region            0\n",
      "Product ID        0\n",
      "Category          0\n",
      "Sub-Category      0\n",
      "Product Name      0\n",
      "Sales             0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Rows: 0\n",
      "\n",
      "Number of Sales Outliers: 1141\n",
      "\n",
      "Unique values in Ship Mode: ['Second Class' 'Standard Class' 'First Class' 'Same Day']\n",
      "\n",
      "Unique values in Segment: ['Consumer' 'Corporate' 'Home Office']\n",
      "\n",
      "Unique values in Country: ['United States']\n",
      "\n",
      "Unique values in City: ['Henderson' 'Los Angeles' 'Fort Lauderdale' 'Concord' 'Seattle'\n",
      " 'Fort Worth' 'Madison' 'West Jordan' 'San Francisco' 'Fremont'\n",
      " 'Philadelphia' 'Orem' 'Houston' 'Richardson' 'Naperville' 'Melbourne'\n",
      " 'Eagan' 'Westland' 'Dover' 'New Albany' 'New York City' 'Troy' 'Chicago'\n",
      " 'Gilbert' 'Springfield' 'Jackson' 'Memphis' 'Decatur' 'Durham' 'Columbia'\n",
      " 'Rochester' 'Minneapolis' 'Portland' 'Saint Paul' 'Aurora' 'Charlotte'\n",
      " 'Orland Park' 'Urbandale' 'Columbus' 'Bristol' 'Wilmington' 'Bloomington'\n",
      " 'Phoenix' 'Roseville' 'Independence' 'Pasadena' 'Newark' 'Franklin'\n",
      " 'Scottsdale' 'San Jose' 'Edmond' 'Carlsbad' 'San Antonio' 'Monroe'\n",
      " 'Fairfield' 'Grand Prairie' 'Redlands' 'Hamilton' 'Westfield' 'Akron'\n",
      " 'Denver' 'Dallas' 'Whittier' 'Saginaw' 'Medina' 'Dublin' 'Detroit'\n",
      " 'Tampa' 'Santa Clara' 'Lakeville' 'San Diego' 'Brentwood' 'Chapel Hill'\n",
      " 'Morristown' 'Cincinnati' 'Inglewood' 'Tamarac' 'Colorado Springs'\n",
      " 'Belleville' 'Taylor' 'Lakewood' 'Arlington' 'Arvada' 'Hackensack'\n",
      " 'Saint Petersburg' 'Long Beach' 'Hesperia' 'Murfreesboro' 'Layton'\n",
      " 'Austin' 'Lowell' 'Manchester' 'Harlingen' 'Tucson' 'Quincy'\n",
      " 'Pembroke Pines' 'Des Moines' 'Peoria' 'Las Vegas' 'Warwick' 'Miami'\n",
      " 'Huntington Beach' 'Richmond' 'Louisville' 'Lawrence' 'Canton'\n",
      " 'New Rochelle' 'Gastonia' 'Jacksonville' 'Auburn' 'Norman' 'Park Ridge'\n",
      " 'Amarillo' 'Lindenhurst' 'Huntsville' 'Fayetteville' 'Costa Mesa'\n",
      " 'Parker' 'Atlanta' 'Gladstone' 'Great Falls' 'Lakeland' 'Montgomery'\n",
      " 'Mesa' 'Green Bay' 'Anaheim' 'Marysville' 'Salem' 'Laredo' 'Grove City'\n",
      " 'Dearborn' 'Warner Robins' 'Vallejo' 'Mission Viejo' 'Rochester Hills'\n",
      " 'Plainfield' 'Sierra Vista' 'Vancouver' 'Cleveland' 'Tyler' 'Burlington'\n",
      " 'Waynesboro' 'Chester' 'Cary' 'Palm Coast' 'Mount Vernon' 'Hialeah'\n",
      " 'Oceanside' 'Evanston' 'Trenton' 'Cottage Grove' 'Bossier City'\n",
      " 'Lancaster' 'Asheville' 'Lake Elsinore' 'Omaha' 'Edmonds' 'Santa Ana'\n",
      " 'Milwaukee' 'Florence' 'Lorain' 'Linden' 'Salinas' 'New Brunswick'\n",
      " 'Garland' 'Norwich' 'Alexandria' 'Toledo' 'Farmington' 'Riverside'\n",
      " 'Torrance' 'Round Rock' 'Boca Raton' 'Virginia Beach' 'Murrieta'\n",
      " 'Olympia' 'Washington' 'Jefferson City' 'Saint Peters' 'Rockford'\n",
      " 'Brownsville' 'Yonkers' 'Oakland' 'Clinton' 'Encinitas' 'Roswell'\n",
      " 'Jonesboro' 'Antioch' 'Homestead' 'La Porte' 'Lansing' 'Cuyahoga Falls'\n",
      " 'Reno' 'Harrisonburg' 'Escondido' 'Royal Oak' 'Rockville' 'Coral Springs'\n",
      " 'Buffalo' 'Boynton Beach' 'Gulfport' 'Fresno' 'Greenville' 'Macon'\n",
      " 'Cedar Rapids' 'Providence' 'Pueblo' 'Deltona' 'Murray' 'Middletown'\n",
      " 'Freeport' 'Pico Rivera' 'Provo' 'Pleasant Grove' 'Smyrna' 'Parma'\n",
      " 'Mobile' 'New Bedford' 'Irving' 'Vineland' 'Glendale' 'Niagara Falls'\n",
      " 'Thomasville' 'Westminster' 'Coppell' 'Pomona' 'North Las Vegas'\n",
      " 'Allentown' 'Tempe' 'Laguna Niguel' 'Bridgeton' 'Everett' 'Watertown'\n",
      " 'Appleton' 'Bellevue' 'Allen' 'El Paso' 'Grapevine' 'Carrollton' 'Kent'\n",
      " 'Lafayette' 'Tigard' 'Skokie' 'Plano' 'Suffolk' 'Indianapolis' 'Bayonne'\n",
      " 'Greensboro' 'Baltimore' 'Kenosha' 'Olathe' 'Tulsa' 'Redmond' 'Raleigh'\n",
      " 'Muskogee' 'Meriden' 'Bowling Green' 'South Bend' 'Spokane' 'Keller'\n",
      " 'Port Orange' 'Medford' 'Charlottesville' 'Missoula' 'Apopka' 'Reading'\n",
      " 'Broomfield' 'Paterson' 'Oklahoma City' 'Chesapeake' 'Lubbock'\n",
      " 'Johnson City' 'San Bernardino' 'Leominster' 'Bozeman' 'Perth Amboy'\n",
      " 'Ontario' 'Rancho Cucamonga' 'Moorhead' 'Mesquite' 'Stockton'\n",
      " 'Ormond Beach' 'Sunnyvale' 'York' 'College Station' 'Saint Louis'\n",
      " 'Manteca' 'San Angelo' 'Salt Lake City' 'Knoxville' 'Little Rock'\n",
      " 'Lincoln Park' 'Marion' 'Littleton' 'Bangor' 'Southaven' 'New Castle'\n",
      " 'Midland' 'Sioux Falls' 'Fort Collins' 'Clarksville' 'Sacramento'\n",
      " 'Thousand Oaks' 'Malden' 'Holyoke' 'Albuquerque' 'Sparks' 'Coachella'\n",
      " 'Elmhurst' 'Passaic' 'North Charleston' 'Newport News' 'Jamestown'\n",
      " 'Mishawaka' 'La Quinta' 'Tallahassee' 'Nashville' 'Bellingham'\n",
      " 'Woodstock' 'Haltom City' 'Wheeling' 'Summerville' 'Hot Springs'\n",
      " 'Englewood' 'Las Cruces' 'Hoover' 'Frisco' 'Vacaville' 'Waukesha'\n",
      " 'Bakersfield' 'Pompano Beach' 'Corpus Christi' 'Redondo Beach' 'Orlando'\n",
      " 'Orange' 'Lake Charles' 'Highland Park' 'Hempstead' 'Noblesville'\n",
      " 'Apple Valley' 'Mount Pleasant' 'Sterling Heights' 'Eau Claire' 'Pharr'\n",
      " 'Billings' 'Gresham' 'Chattanooga' 'Meridian' 'Bolingbrook' 'Maple Grove'\n",
      " 'Woodland' 'Missouri City' 'Pearland' 'San Mateo' 'Grand Rapids'\n",
      " 'Visalia' 'Overland Park' 'Temecula' 'Yucaipa' 'Revere' 'Conroe'\n",
      " 'Tinley Park' 'Dubuque' 'Dearborn Heights' 'Santa Fe' 'Hickory'\n",
      " 'Carol Stream' 'Saint Cloud' 'North Miami' 'Plantation'\n",
      " 'Port Saint Lucie' 'Rock Hill' 'Odessa' 'West Allis' 'Chula Vista'\n",
      " 'Manhattan' 'Altoona' 'Thornton' 'Champaign' 'Texarkana' 'Edinburg'\n",
      " 'Baytown' 'Greenwood' 'Woonsocket' 'Superior' 'Bedford' 'Covington'\n",
      " 'Broken Arrow' 'Miramar' 'Hollywood' 'Deer Park' 'Wichita' 'Mcallen'\n",
      " 'Iowa City' 'Boise' 'Cranston' 'Port Arthur' 'Citrus Heights'\n",
      " 'The Colony' 'Daytona Beach' 'Bullhead City' 'Portage' 'Fargo' 'Elkhart'\n",
      " 'San Gabriel' 'Margate' 'Sandy Springs' 'Mentor' 'Lawton' 'Hampton'\n",
      " 'Rome' 'La Crosse' 'Lewiston' 'Hattiesburg' 'Danville' 'Logan'\n",
      " 'Waterbury' 'Athens' 'Avondale' 'Marietta' 'Yuma' 'Wausau' 'Pasco'\n",
      " 'Oak Park' 'Pensacola' 'League City' 'Gaithersburg' 'Lehi' 'Tuscaloosa'\n",
      " 'Moreno Valley' 'Georgetown' 'Loveland' 'Chandler' 'Helena' 'Kirkwood'\n",
      " 'Waco' 'Frankfort' 'Bethlehem' 'Grand Island' 'Woodbury' 'Rogers'\n",
      " 'Clovis' 'Jupiter' 'Santa Barbara' 'Cedar Hill' 'Norfolk' 'Draper'\n",
      " 'Ann Arbor' 'La Mesa' 'Pocatello' 'Holland' 'Milford' 'Buffalo Grove'\n",
      " 'Lake Forest' 'Redding' 'Chico' 'Utica' 'Conway' 'Cheyenne' 'Owensboro'\n",
      " 'Caldwell' 'Kenner' 'Nashua' 'Bartlett' 'Redwood City' 'Lebanon'\n",
      " 'Santa Maria' 'Des Plaines' 'Longview' 'Hendersonville' 'Waterloo'\n",
      " 'Cambridge' 'Palatine' 'Beverly' 'Eugene' 'Oxnard' 'Renton' 'Glenview'\n",
      " 'Delray Beach' 'Commerce City' 'Texas City' 'Wilson' 'Rio Rancho'\n",
      " 'Goldsboro' 'Montebello' 'El Cajon' 'Beaumont' 'West Palm Beach'\n",
      " 'Abilene' 'Normal' 'Saint Charles' 'Camarillo' 'Hillsboro' 'Burbank'\n",
      " 'Modesto' 'Garden City' 'Atlantic City' 'Longmont' 'Davis' 'Morgan Hill'\n",
      " 'Clifton' 'Sheboygan' 'East Point' 'Rapid City' 'Andover' 'Kissimmee'\n",
      " 'Shelton' 'Danbury' 'Sanford' 'San Marcos' 'Greeley' 'Mansfield' 'Elyria'\n",
      " 'Twin Falls' 'Coral Gables' 'Romeoville' 'Marlborough' 'Laurel' 'Bryan'\n",
      " 'Pine Bluff' 'Aberdeen' 'Hagerstown' 'East Orange' 'Arlington Heights'\n",
      " 'Oswego' 'Coon Rapids' 'San Clemente' 'San Luis Obispo' 'Springdale']\n",
      "\n",
      "Unique values in State: ['Kentucky' 'California' 'Florida' 'North Carolina' 'Washington' 'Texas'\n",
      " 'Wisconsin' 'Utah' 'Nebraska' 'Pennsylvania' 'Illinois' 'Minnesota'\n",
      " 'Michigan' 'Delaware' 'Indiana' 'New York' 'Arizona' 'Virginia'\n",
      " 'Tennessee' 'Alabama' 'South Carolina' 'Oregon' 'Colorado' 'Iowa' 'Ohio'\n",
      " 'Missouri' 'Oklahoma' 'New Mexico' 'Louisiana' 'Connecticut' 'New Jersey'\n",
      " 'Massachusetts' 'Georgia' 'Nevada' 'Rhode Island' 'Mississippi'\n",
      " 'Arkansas' 'Montana' 'New Hampshire' 'Maryland' 'District of Columbia'\n",
      " 'Kansas' 'Maine' 'South Dakota' 'Idaho' 'North Dakota' 'Wyoming'\n",
      " 'West Virginia']\n",
      "\n",
      "Unique values in Region: ['South' 'West' 'Central' 'East']\n",
      "\n",
      "Unique values in Category: ['Furniture' 'Office Supplies' 'Technology']\n",
      "\n",
      "Unique values in Sub-Category: ['Bookcases' 'Chairs' 'Labels' 'Tables' 'Storage' 'Furnishings' 'Art'\n",
      " 'Phones' 'Binders' 'Appliances' 'Paper' 'Accessories' 'Envelopes'\n",
      " 'Fasteners' 'Supplies' 'Machines' 'Copiers']\n",
      "\n",
      "Invalid Ship Dates (Ship Date < Order Date): 0\n",
      "\n",
      "Cleaned dataset saved as 'superstore_sales_cleaned.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('C:\\\\Users\\\\mooda\\\\OneDrive\\\\Desktop\\\\superstore_sales.csv')\n",
    "\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nFirst 5 Rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "df = df.dropna(subset=['Order ID', 'Customer ID', 'Product ID', 'Sales', 'Postal Code'])\n",
    "\n",
    "print(\"\\nDuplicate Rows:\", df.duplicated().sum())\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df['Order Date'] = pd.to_datetime(df['Order Date'], format='%d/%m/%Y', errors='coerce')\n",
    "df['Ship Date'] = pd.to_datetime(df['Ship Date'], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "categorical_cols = ['Ship Mode', 'Segment', 'Country', 'City', 'State', 'Region', 'Category', 'Sub-Category']\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "df['Sales'] = df['Sales'].astype(float)\n",
    "df['Postal Code'] = df['Postal Code'].astype(int)\n",
    "\n",
    "Q1 = df['Sales'].quantile(0.25)\n",
    "Q3 = df['Sales'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "outliers = df[(df['Sales'] < lower_bound) | (df['Sales'] > upper_bound)]\n",
    "print(\"\\nNumber of Sales Outliers:\", len(outliers))\n",
    "df['Sales'] = df['Sales'].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\nUnique values in {col}:\", df[col].unique())\n",
    "df['Ship Mode'] = df['Ship Mode'].replace({'Second Class': 'Second Class', 'Standard Class': 'Standard Class', 'First Class': 'First Class', 'Same Day': 'Same Day'})\n",
    "\n",
    "invalid_dates = df[df['Ship Date'] < df['Order Date']]\n",
    "print(\"\\nInvalid Ship Dates (Ship Date < Order Date):\", len(invalid_dates))\n",
    "df = df[df['Ship Date'] >= df['Order Date']]\n",
    "\n",
    "df.to_csv('superstore_sales_cleaned.csv', index=False)\n",
    "print(\"\\nCleaned dataset saved as 'superstore_sales_cleaned.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5da5e681-6620-4230-a716-ab00011d3903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      "Row ID           0\n",
      "Order ID         0\n",
      "Order Date       0\n",
      "Ship Date        0\n",
      "Ship Mode        0\n",
      "Customer ID      0\n",
      "Customer Name    0\n",
      "Segment          0\n",
      "Country          0\n",
      "City             0\n",
      "State            0\n",
      "Postal Code      0\n",
      "Region           0\n",
      "Product ID       0\n",
      "Category         0\n",
      "Sub-Category     0\n",
      "Product Name     0\n",
      "Sales            0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mooda\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:246: UserWarning: Found unknown categories in columns [3] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "C:\\Users\\mooda\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:246: UserWarning: Found unknown categories in columns [3] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "C:\\Users\\mooda\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:246: UserWarning: Found unknown categories in columns [3] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Performance:\n",
      "Confusion Matrix:\n",
      "[[47  4]\n",
      " [ 5 10]]\n",
      "Accuracy: 0.863636363636\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91        51\n",
      "           1       0.71      0.67      0.69        15\n",
      "\n",
      "    accuracy                           0.86        66\n",
      "   macro avg       0.81      0.79      0.80        66\n",
      "weighted avg       0.86      0.86      0.86        66\n",
      "\n",
      "\n",
      "SVR Performance:\n",
      "Confusion Matrix:\n",
      "[[45  6]\n",
      " [ 5 10]]\n",
      "Accuracy: 0.833333333333\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89        51\n",
      "           1       0.62      0.67      0.65        15\n",
      "\n",
      "    accuracy                           0.83        66\n",
      "   macro avg       0.76      0.77      0.77        66\n",
      "weighted avg       0.84      0.83      0.84        66\n",
      "\n",
      "\n",
      "KNN Performance:\n",
      "Confusion Matrix:\n",
      "[[49  2]\n",
      " [ 8  7]]\n",
      "Accuracy: 0.848484848485\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91        51\n",
      "           1       0.78      0.47      0.58        15\n",
      "\n",
      "    accuracy                           0.85        66\n",
      "   macro avg       0.82      0.71      0.75        66\n",
      "weighted avg       0.84      0.85      0.83        66\n",
      "\n",
      "\n",
      "Preprocessed dataset saved as 'superstore_sales_preprocessed_combined.csv'\n",
      "Random Forest confusion matrix saved as 'rf_confusion_matrix.png'\n",
      "SVR confusion matrix saved as 'svr_confusion_matrix.png'\n",
      "KNN confusion matrix saved as 'knn_confusion_matrix.png'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "print(\"Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n",
    "df = df.dropna(subset=['Order ID', 'Customer ID', 'Product ID', 'Sales', 'Postal Code'])\n",
    "\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "\n",
    "df['Order Date'] = pd.to_datetime(df['Order Date'], format='%d/%m/%Y', errors='coerce')\n",
    "df['Ship Date'] = pd.to_datetime(df['Ship Date'], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "\n",
    "df = df[df['Ship Date'] >= df['Order Date']]\n",
    "\n",
    "\n",
    "df['Shipping Duration'] = (df['Ship Date'] - df['Order Date']).dt.days\n",
    "df['Customer Order Count'] = df.groupby('Customer ID')['Order ID'].transform('count')\n",
    "\n",
    "\n",
    "df['Order Year'] = df['Order Date'].dt.year\n",
    "df['Order Month'] = df['Order Date'].dt.month\n",
    "df['Order Day'] = df['Order Date'].dt.day\n",
    "df['Ship Year'] = df['Ship Date'].dt.year\n",
    "df['Ship Month'] = df['Ship Date'].dt.month\n",
    "df['Ship Day'] = df['Ship Date'].dt.day\n",
    "\n",
    "\n",
    "df = df.drop(['Order Date', 'Ship Date', 'Row ID', 'Order ID', 'Customer Name', 'Product ID', 'Product Name'], axis=1)\n",
    "\n",
    "\n",
    "categorical_cols = ['Ship Mode', 'Segment', 'Country', 'City', 'State', 'Region', 'Category', 'Sub-Category']\n",
    "numerical_cols = ['Postal Code', 'Order Year', 'Order Month', 'Order Day', 'Ship Year', 'Ship Month', 'Ship Day', 'Shipping Duration', 'Customer Order Count']\n",
    "\n",
    "\n",
    "sales_median = df['Sales'].median()\n",
    "df['High_Sales'] = (df['Sales'] > sales_median).astype(int)\n",
    "X = df.drop(['Sales', 'High_Sales', 'Customer ID'], axis=1)\n",
    "y = df['High_Sales'] \n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), categorical_cols)\n",
    "    ])\n",
    "\n",
    "\n",
    "X_temp, _, y_temp, _ = train_test_split(X, y, train_size=1000, stratify=y, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_temp, y_temp, test_size=66, stratify=y_temp, random_state=42)\n",
    "\n",
    "\n",
    "test_df = pd.DataFrame(X_test).copy()\n",
    "test_df['High_Sales'] = y_test\n",
    "class_0 = test_df[test_df['High_Sales'] == 0]\n",
    "class_1 = test_df[test_df['High_Sales'] == 1]\n",
    "class_0_sampled = resample(class_0, n_samples=51, random_state=42)\n",
    "class_1_sampled = resample(class_1, n_samples=15, random_state=42)\n",
    "test_df_balanced = pd.concat([class_0_sampled, class_1_sampled])\n",
    "X_test = test_df_balanced.drop('High_Sales', axis=1)\n",
    "y_test = test_df_balanced['High_Sales']\n",
    "\n",
    "\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=5, random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_rf = rf_pipeline.predict(X_test)\n",
    "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "class_report_rf = classification_report(y_test, y_pred_rf)\n",
    "\n",
    "\n",
    "print(\"\\nRandom Forest Performance:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_rf)\n",
    "print(f\"Accuracy: {accuracy_rf:.12f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(class_report_rf)\n",
    "\n",
    "svr_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', SVR(kernel='rbf', C=1, epsilon=0.1))\n",
    "])\n",
    "\n",
    "\n",
    "svr_pipeline.fit(X_train, df.loc[X_train.index, 'Sales'])\n",
    "\n",
    "\n",
    "y_pred_svr = svr_pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "y_pred_svr_binary = (y_pred_svr > sales_median).astype(int)\n",
    "\n",
    "conf_matrix_svr = confusion_matrix(y_test, y_pred_svr_binary)\n",
    "accuracy_svr = accuracy_score(y_test, y_pred_svr_binary)\n",
    "class_report_svr = classification_report(y_test, y_pred_svr_binary)\n",
    "\n",
    "print(\"\\nSVR Performance:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_svr)\n",
    "print(f\"Accuracy: {accuracy_svr:.12f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(class_report_svr)\n",
    "\n",
    "\n",
    "knn_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', KNeighborsClassifier(n_neighbors=5))\n",
    "])\n",
    "\n",
    "knn_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred_knn = knn_pipeline.predict(X_test)\n",
    "conf_matrix_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "class_report_knn = classification_report(y_test, y_pred_knn)\n",
    "\n",
    "print(\"\\nKNN Performance:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_knn)\n",
    "print(f\"Accuracy: {accuracy_knn:.12f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(class_report_knn)\n",
    "\n",
    "\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "preprocessed_columns = numerical_cols + list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols))\n",
    "df_preprocessed = pd.DataFrame(X_preprocessed, columns=preprocessed_columns)\n",
    "df_preprocessed['High_Sales'] = y.reset_index(drop=True)\n",
    "df_preprocessed.to_csv('superstore_sales_preprocessed_combined.csv', index=False)\n",
    "print(\"\\nPreprocessed dataset saved as 'superstore_sales_preprocessed_combined.csv'\")\n",
    "print(\"Random Forest confusion matrix saved as 'rf_confusion_matrix.png'\")\n",
    "print(\"SVR confusion matrix saved as 'svr_confusion_matrix.png'\")\n",
    "print(\"KNN confusion matrix saved as 'knn_confusion_matrix.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f44ccd-88bd-4702-ac2b-b18a84aad803",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
